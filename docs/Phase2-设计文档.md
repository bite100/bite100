# P2P 去中心化交易所 - Phase 2 设计文档

> 版本：v0.1  
> 更新日期：2025-02-07  
> 关联文档：[概念设计文档](./概念设计文档.md)、[技术架构说明](./技术架构说明.md)

---

## 一、目标与范围

### 1.1 Phase 2 目标

在 Phase 1（链上托管 + AMM + Web 前端）基础上，建立 **节点网络**：用户可运行节点软件，贡献存储与带宽，参与数据同步，并获得可验证的贡献记录与奖励。

### 1.2 交付范围

| 交付物 | 说明 |
|--------|------|
| **节点软件** | 单一体可执行程序/安装包，支持 Windows / macOS / Linux，可配置为存储节点或中继节点 |
| **数据同步与存储** | 节点间订单簿/成交数据同步协议，存储节点持久化与保留期清理（统一两周） |
| **贡献度量与奖励** | 在线时长、存储量、转发量等指标采集与证明，与链上奖励合约对接（或预留接口） |

### 1.3 不包含（留待 Phase 3）

- 撮合节点与链下订单簿撮合（Phase 3）
- 完整经济模型与代币激励（Phase 3 细化）
- 手机端节点或移动 App（Phase 4）

---

## 二、节点软件设计

### 2.1 定位与形态

- **定位**：贡献者在本机运行的常驻进程，加入 P2P 网络，提供存储或中继能力。
- **形态**：CLI + 可选 TUI/Web 状态页；支持后台运行（systemd / launchd / Windows 服务可选）。
- **与 Phase 1 的关系**：前端 DApp 仍直接连链与合约交互；节点不替代前端，而是为未来的链下订单簿、数据查询提供基础设施。

### 2.2 节点类型（Phase 2 先实现两类）

| 类型 | 职责 | Phase 2 实现重点 |
|------|------|------------------|
| **存储节点** | 持久化订单簿快照、历史成交；响应同步请求；执行保留期清理 | 本地 DB、同步协议服务端、清理任务 |
| **中继节点** | 转发 P2P 消息、维持网络连通性 | libp2p 中继、Gossip 订阅与转发、基础指标上报 |

### 2.3 软件架构

```
┌─────────────────────────────────────────────────────────────────┐
│  Config / CLI / (可选) Web 状态页                                 │
└─────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────┐
│  Node Core                                                        │
│  ├── Identity (密钥、NodeID)                                      │
│  ├── P2P Host (libp2p: DHT, GossipSub, Relay)                    │
│  ├── Sync Protocol (请求/响应 订单簿、成交)                         │
│  └── Metrics Collector (uptime, storage, bytes relayed)           │
└─────────────────────────────────────────────────────────────────┘
                                    │
┌──────────────────┬───────────────────────────────────────────────┐
│  存储节点模块     │  中继节点模块                                    │
│  ├── Local DB    │  ├── Topic 订阅与转发                            │
│  ├── Retention   │  └── 转发量统计                                  │
│  └── Sync Server │                                                  │
└──────────────────┴───────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────┐
│  Blockchain Adapter (只读：读链上状态；可选：提交贡献证明)          │
└─────────────────────────────────────────────────────────────────┘
```

### 2.4 技术选型

| 项目 | 选型 | 说明 |
|------|------|------|
| 实现语言 | **Go** 或 **Rust** | 推荐 Go：libp2p 生态成熟，跨平台构建简单 |
| P2P 栈 | **libp2p** (go-libp2p / rust-libp2p) | 节点发现、DHT、GossipSub、Relay |
| 存储 | **SQLite** 或 **LevelDB** | 单节点足够，便于备份与迁移 |
| 配置 | YAML 或 TOML | 节点类型、Bootstrap 列表、链 RPC、数据目录 |

### 2.5 配置示例

```yaml
# config.yaml
node:
  type: storage   # storage | relay
  data_dir: ./data
  listen:
    - /ip4/0.0.0.0/tcp/4001
    - /ip4/0.0.0.0/udp/4001/quic-v1

network:
  bootstrap:
    - /ip4/1.2.3.4/tcp/4001/p2p/Qm...
  topics:
    - /p2p-exchange/sync/orderbook
    - /p2p-exchange/sync/trades

chain:
  rpc_url: https://sepolia.infura.io/v3/...
  chain_id: 11155111

storage:          # 仅存储节点，0=两周，>0=月数
  retention_months: 0
  snapshot_interval_h: 24
```

### 2.6 目录结构（建议仓库布局）

```
node/                 # 或 p2p-node/
├── cmd/
│   └── node/
│       └── main.go
├── internal/
│   ├── config/
│   ├── p2p/          # libp2p host, gossip, dht
│   ├── sync/         # 同步协议：请求/响应
│   ├── storage/      # DB、保留策略、清理
│   ├── relay/        # 中继逻辑与统计
│   └── metrics/      # 贡献指标采集
├── config.example.yaml
├── go.mod
└── README.md
```

---

## 三、数据同步与存储节点

### 3.1 数据范围（Phase 2）

Phase 1 当前为 AMM，无链下订单簿。Phase 2 数据同步先以「可扩展结构」为主，为 Phase 3 订单簿做准备：

| 数据类型 | 说明 | Phase 2 存储/同步 |
|----------|------|--------------------|
| **订单簿快照** | 交易对、买卖盘深度 | 结构定义 + 同步协议；内容可先为占位或从链上 AMM 状态推导 |
| **历史成交** | 链上已结算的 Swap/成交 | 从链上事件或前端已有逻辑拉取，写入存储节点，用于同步与保留期存储 |
| **贡献证明** | 节点自报的指标 | 见第四节 |

### 3.2 存储节点数据模型

与[技术架构说明](./技术架构说明.md)一致，仅做 Phase 2 可实现子集：

**成交记录（与链上一致）**

```json
{
  "tradeId": "0x...",
  "pair": "TKA/TKB",
  "takerOrderId": "0x...",
  "makerOrderId": "0x...",
  "price": "1.0",
  "amount": "100",
  "fee": "0.1",
  "timestamp": 1707292850,
  "txHash": "0x..."
}
```

**订单簿快照（为 Phase 3 预留）**

```json
{
  "pair": "TKA/TKB",
  "snapshotAt": 1707292800,
  "bids": [[ "price", "quantity" ], ...],
  "asks": [[ "price", "quantity" ], ...]
}
```

### 3.3 同步协议

- **传输**：基于 libp2p 的 Request-Response 或 GossipSub 的专用 Topic。
- **发现**：通过 DHT 或固定 Bootstrap 找到存储节点；可维护「存储节点列表」Topic 供查询。

**请求历史成交（拉取）**

- 消息类型：`SyncTradesRequest { since: unix_ts, until: unix_ts, limit: n }`
- 响应：`SyncTradesResponse { trades: [...] }`
- 约束：存储节点仅返回保留期内的数据（默认两周）；`since < now - retention` 的请求视为 `since = now - retention`。

**订单簿快照（Phase 2 可简化）**

- 可先定义消息格式与 Topic，内容由「占位」或「从链上 AMM 状态生成」填充，便于 Phase 3 直接接入真实订单簿。

### 3.4 数据保留与清理

- **保留期**：统一两周（`storage.retention_months` 为 0 时），或 >0 为月数，超期删除。
- **清理**：各存储节点定时任务（如每日）按本节点保留月数删除超期成交与订单簿快照。
- **调取**：手机端需超过 1 个月数据时，向电脑端节点（最多 6 个月）发起 SyncTrades 拉取。

---

## 四、贡献度量与奖励

### 4.1 Phase 2 度量指标

| 指标 | 适用节点 | 采集方式 | 周期 |
|------|----------|----------|------|
| **在线时长 (uptime)** | 全部 | 心跳 + 自报或抽查 | 每 5–10 分钟 |
| **存储容量 (storage)** | 存储节点 | 自报已用/总容量 | 每小时 |
| **转发量 (bytes relayed)** | 中继节点 | 统计转发字节数 | 每 10 分钟 |

### 4.2 贡献证明结构

与[技术架构说明](./技术架构说明.md)一致，扩展为可落地的字段：

```json
{
  "nodeId": "0x...",
  "nodeType": "storage",
  "period": "2025-02-01_2025-02-07",
  "metrics": {
    "uptime": 0.95,
    "storageUsedGB": 10,
    "storageTotalGB": 100,
    "bytesRelayed": 1073741824
  },
  "signature": "0x...",
  "timestamp": 1707292850
}
```

- `nodeId`：节点公钥或派生 ID。  
- `signature`：节点私钥对 `period + metrics` 的签名，供链上或审计方验证。

### 4.3 分配规则：周期内总量固定、按贡献分

- **周期内全网数量固定**：每个周期（如每周）的奖励/分配总量由协议或链上合约事先约定，不随参与节点数量变化。
- **按贡献分配**：该周期内所有参与节点共享该总量，按各自贡献占比分配：
  - **节点 i 所得 = 周期总量 × (节点 i 的贡献值 / 周期内所有参与节点的贡献值之和)**  
- **贡献值**：由 4.1 的指标综合得出（如 uptime、storage、bytes relayed 加权或归一后合并为单一贡献分）；仅在该周期内产生过有效证明的节点计为「参与节点」，纳入分母。
- 链上合约（如 ContributorReward）在周期结束后汇总该周期内收到的有效证明，计算各节点占比并发放对应份额；链下收集时同理，由审计方按同一规则计算分配结果。

### 4.3.1 奖励池与兑换规则（每周手续费 → 稳定币计价 → 多币种兑换）

- **结算滞后一周**：本周获得的贡献对应**上一周**的结算；奖励池价值及各币种对稳定币的价值均按**上一周**核算（即贡献周期与结算/计价周期错后一周）。
- **每周奖励来源**：当周手续费收入，**价值换算成稳定币**作为当周奖励池的「价值总量」；**不做实际货币兑换**，仅用稳定币作为计价单位。
- **储备比例**：奖励池**保留一定比例的储备**（如 10%～20% 不参与当周分配），用于应对波动、协议发展或应急；仅「可分配部分」参与当周每份贡献价值的计算。
- **每份贡献价值**：**当周可分配奖励池价值（稳定币）÷ 当周各节点贡献之和** = 每单位贡献对应多少稳定币价值。
- **兑换方式**：节点用**贡献**兑换奖励池里的**各种货币**（如 USDC、ETH、项目代币等）。兑换时按**该币种在兑换时刻对稳定币的价值**换算：  
  - 节点应得 = 自己的贡献 × 每份贡献价值（稳定币）；  
  - 若选择用代币 A 领取，则领取数量 = 应得稳定币价值 ÷ 代币 A 在**兑换时**的稳定币单价。  
- **价值基准与锁定**：每周的贡献价值**只能按当周各币种对稳定币的平均价格**核算；兑换时（含日后任何时刻）**只能领取价值等于该周核算额度的资产**，即应得额度固定、不可超额领取，只能兑换「这些价值」对应的货币。
- **结算时价格的计算（避免周末单点波动）**：奖励池及各币种在周末结算时若只取某一时刻价格，可能因波动导致价值相差巨大。采用**按日去极值再周平均**：对每个自然日，先去掉该日采样价中**最高的 5%** 与**最低的 5%**，对剩余价格取算术平均得到该日「日均价」；再对当周各日的日均价取**算术平均**，作为当周「平均价格」用于结算。
- **自由流动额度**：允许**当周奖励的一成**（10%）自由流动到以后周期；该部分可按**兑换时**的市场价值兑换货币，不受当周平均价格锁定。
- **效果**：奖励池可持有多种资产，节点按贡献占比获得「稳定币等值」的额度，领取时自选币种、按当时市价折算，无需事先把手续费兑成单一稳定币。

### 4.4 链上对接（Phase 2 可选）

- Phase 1 已有 **FeeDistributor**；技术架构中规划 **ContributorReward**。
- Phase 2 可二选一：
  - **方案 A**：实现 **ContributorReward** 合约（或扩展 FeeDistributor），节点定期提交贡献证明，链上验证签名后累计积分或发放奖励。
  - **方案 B**：链下收集证明、落库或公开 API，链上合约暂不实现，仅预留接口与事件格式，Phase 3 再接入经济模型。

推荐 Phase 2 先做 **链下证明收集 + 可验证结构**，再视资源实现 **ContributorReward** 或扩展 FeeDistributor。**贡献奖励的接口定义（提交证明入参、链上校验与贡献分、ContributorReward 与 FeeDistributor 取舍）见 [贡献奖励接口.md](./贡献奖励接口.md)**。

### 4.5 防作弊要点

- **uptime**：由多节点或 Bootstrap 定期发起探测请求，结合自报做交叉验证。  
- **storage**：抽查：随机请求历史数据片段，验证节点是否真实持有并正确返回。  
- **bytes relayed**：基于 libp2p 的流量统计，可加签名或 nonce 防重放。

---

## 五、里程碑与任务拆解

### M1：节点骨架与 P2P 连通（约 4–6 周）

- [x] 新建 `node/` 目录，Go 工程初始化（[node/README.md](../node/README.md)）
- [x] 集成 libp2p：Host、 listen、connect 两节点互通
- [x] 配置文件加载（YAML）、DHT、GossipSub Topic 订阅
- [x] 节点身份：自动生成密钥，导出 NodeID
- [x] 验收：两节点启动后可通过 `-connect` 连接并维持连接

### M2：存储节点与数据层（约 4–6 周）

- [x] 本地 DB：成交表、订单簿快照表（可为空表），按时间索引。
- [x] 保留期清理：定时清理任务（默认两周），同步接口只返回保留期内数据。
- [x] 同步协议：`SyncTradesRequest/Response` 及 Request-Response 实现（`/p2p-exchange/sync/trades/1.0.0`）。
- [x] （可选）从链上事件或现有接口拉取历史成交写入 DB，供同步使用。
- [x] 验收：存储节点 A 写入数据（`-seed-trades` 插入 5 条测试成交），节点 B 通过 `-sync-from` 拉取到一致结果（见 [node/README.md](../node/README.md) M2 验收）。

### M3：中继与贡献指标（约 3–4 周）

- [x] 中继节点：订阅指定 Topic，转发消息并统计 bytes relayed。
- [x] 指标采集：全部节点 uptime；存储节点 storage（已用/总容量）；中继节点 bytes relayed。
- [x] 贡献证明：按周期（如每周）生成带签名的证明结构，落盘或发到指定 Topic/API（当前以落盘为主）。
- [x] 验收：连续运行约 24 小时，能产出符合格式的贡献证明（见 [node/README.md](../node/README.md) M3 验收）。

### M4：链上对接与文档（约 2–4 周）

- [x] 定义 ContributorReward 或扩展 FeeDistributor 的接口（提交证明、查询积分/奖励）；**接口见 [贡献奖励接口.md](./贡献奖励接口.md)**。
- [x] （可选）实现并部署合约，节点端提交证明的调用或脚本（ContributorReward 已部署，submitproof 已实现）。
- [x] 节点部署文档：安装、配置、Bootstrap 列表、防火墙建议（见 [节点部署.md](./节点部署.md)）。
- [x] 更新[概念设计文档](./概念设计文档.md) Phase 2 勾选项与本文档链接。

**建议执行顺序（下一步）**：

| 顺序 | 任务 | 说明 |
|------|------|------|
| 1 | **节点部署文档** | 与合约无关，可先做：安装方式（Go/Docker）、config 说明、Bootstrap、防火墙/端口建议，放在 `docs/节点部署.md` 或 node 目录。 |
| 2 | **定义贡献奖励接口** | 已写草稿 [贡献奖励接口.md](./贡献奖励接口.md)：提交证明入参（含 ECDSA 方案）、链上校验与贡献分公式、推荐新增 ContributorReward 不扩展 FeeDistributor；可在此基础上定稿并实现合约。 |
| 3 | **（可选）合约 + 节点提交** | 实现合约与节点端调用（如 Go 脚本读 proof JSON、调合约 submitProof），或先只做链下收集与 4.3 分配规则计算。 |
| 4 | **更新概念设计文档** | 在[概念设计文档](./概念设计文档.md)中勾选 Phase 2 已完成项并链到本文档。 |

---

## 六、风险与依赖

| 风险 | 缓解 |
|------|------|
| libp2p 学习曲线 | 先用 go-libp2p 官方示例打通 Host + DHT + GossipSub，再扩展协议 |
| NAT 穿透 | 使用 libp2p Relay + AutoNAT；必要时预留公网 Bootstrap 节点 |
| 贡献证明被伪造 | 签名校验、链上或中心化审计；Phase 2 可先链下审计 |
| 与 Phase 1 前端脱节 | 节点不替代前端；前端仍直连链；节点仅作为未来「订单簿/历史查询」的数据源，接口在 Phase 3 再统一 |

**依赖**：Phase 1 合约与前端稳定；Sepolia（或目标链）RPC 可用；Bootstrap 节点需至少 1–2 个可公开访问的节点（可项目方先部署）。

---

## 七、附录

### 7.1 参考

- [go-libp2p 示例](https://github.com/libp2p/go-libp2p/tree/master/examples)
- [GossipSub 规范](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.0.md)
- [技术架构说明 - 存储节点数据模型](./技术架构说明.md#42-存储节点数据模型)

### 7.2 文档历史

| 版本 | 日期 | 说明 |
|------|------|------|
| v0.1 | 2025-02-07 | 初稿：节点软件、数据同步、贡献度量与里程碑 |

---

*Phase 2 设计以当时修改为准，随实现推进随时更新设计文档；参见 [设计文档索引](./设计文档索引.md)。*
